{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unimodal network of who-knows-who\n",
    "## diary= DataFrame -- from hand typed csv\n",
    "## path= the storage path\n",
    "## export a csv of nodes, a csv of adjacency matrix, and a csv of edges\n",
    "## return\n",
    "def ppl(diary):\n",
    "    ## clean the data\n",
    "    diary.dropna(axis=[0,1],how='all',inplace=True)\n",
    "    diary['Participants']=StdNm(diary['Participants'])\n",
    "    \n",
    "    ## make people-event DataFrame\n",
    "    ppl_event=diary[['Type','Participants']].dropna(subset=['Participants'])\n",
    "    \n",
    "    #### make event index\n",
    "    event_iter=0\n",
    "    event_idx=[]\n",
    "    for boo in ppl_event['Type'].notnull():\n",
    "        if boo:\n",
    "            event_iter+=1\n",
    "        event_idx.append(event_iter)\n",
    "    \n",
    "    ppl_event[\"Event\"]=event_idx\n",
    "    \n",
    "    ## make a series of unique names as the index and columns of the adjacency matrix\n",
    "    ppl_uniq=pd.Series(ppl_event['Participants'].value_counts().index).append(pd.Series(\"謝蘭生\"),ignore_index=True)\n",
    "    ppl_df=pd.DataFrame(ppl_uniq).set_index(0)\n",
    "    \n",
    "    ## initialize the adjacency matrix\n",
    "    AdjMa=ppl_df.join(ppl_df.T,how='outer').fillna(0)\n",
    "    \n",
    "    ## Count frequency from event\n",
    "    event=ppl_event.set_index('Event').drop('Type',axis=1)\n",
    "    for i in np.arange(1,event_iter):\n",
    "        if len(event.ix[i])>1:\n",
    "            idx=event.ix[i]['Participants']\n",
    "            AdjMa.ix[idx,idx]+=1\n",
    "        i+=1\n",
    "    #### diagonal -- 0\n",
    "    np.fill_diagonal(AdjMa.values,0)\n",
    "    #### for 謝蘭生\n",
    "    freq=np.append(ppl_event['Participants'].value_counts().values,0)\n",
    "    AdjMa[\"謝蘭生\"]=freq\n",
    "    AdjMa.ix[\"謝蘭生\"]=freq\n",
    "    ## export a csv of Adjacency Matrix\n",
    "    AdjMa.to_csv('pplAdj.csv')\n",
    "    \n",
    "    ## export a csv of people as nodes\n",
    "    ppl_df=ppl_df.reset_index().reset_index().rename(columns={0:\"Label\",\"index\":\"Id\"})\n",
    "    ppl_df.to_csv('pplNodes.csv',index=False)\n",
    "    \n",
    "    ## make edgelist\n",
    "    AdjMa=AdjMa.replace(0,np.nan)\n",
    "    \n",
    "    i=0\n",
    "    Source=[]\n",
    "    Target=[]\n",
    "    Weight=[]\n",
    "    for colName,ser in AdjMa.iteritems():\n",
    "        # symmetric matrix --> triangle\n",
    "        prep=ser[:i][pd.notnull(ser)]\n",
    "        for idx,value in prep.iteritems():\n",
    "            #print colName,idx,value\n",
    "            Source.append(colName)\n",
    "            Target.append(idx)\n",
    "            Weight.append(value)\n",
    "        i+=1\n",
    "    \n",
    "    #source=pd.merge(pd.DataFrame(Source),ppl_df,how='left',left_on=0,right_on=\"Label\")[\"Id\"]\n",
    "    #target=pd.merge(pd.DataFrame(Target),ppl_df,how='left',left_on=0,right_on=\"Label\")[\"Id\"]\n",
    "    edge=pd.DataFrame({\"Id\":np.arange(len(Weight)),\"Source\":source,\"Target\":target,\\\n",
    "                       \"Type\":len(Weight)*[\"Undirected\"],\"Weight\":Weight})\n",
    "    edge.to_csv('pplEdges.csv',index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# people-place bimodal network\n",
    "## diary = DataFrame -- from hand typed csv\n",
    "## path = the storage path\n",
    "## export a csv of people-place nodes and a csv of people-place edges\n",
    "## return\n",
    "def ppl_plc(diary):\n",
    "    \n",
    "    ## clean the data\n",
    "    diary.dropna(axis=[0,1],how='all',inplace=True)\n",
    "    diary['Participants']=StdNm(diary['Participants'])\n",
    "    \n",
    "    #### make nodes\n",
    "    ppl_plc=diary[[\"Place\",\"Participants\"]].dropna()\n",
    "    ppl=pd.DataFrame({\"Label\":ppl_plc[\"Participants\"].unique(),\"Modularity Class\":\"People\"})\n",
    "    plc=pd.DataFrame({\"Label\":ppl_plc[\"Place\"].unique(),\"Modularity Class\":\"Place\"})\n",
    "    binodes=pd.DataFrame(ppl.append(plc,ignore_index=True))\n",
    "    binodes=binodes.reset_index().rename(columns={\"index\":\"Id\"})\n",
    "    binodes.to_csv(\"biNodes.csv\",index=False)\n",
    "    \n",
    "    #### make edges\n",
    "    target=pd.merge(ppl_plc,binodes,how='left',left_on=\"Place\",right_on=\"Label\")[\"Id\"]\n",
    "    source=pd.merge(ppl_plc,binodes,how='left',left_on=\"Participants\",right_on=\"Label\")[\"Id\"]\n",
    "    edges=pd.DataFrame({\"Id\":np.arange(len(source)),\"Source\":source,\"Target\":target,\"Weight\":1})\n",
    "    edges=edges.groupby([\"Source\",\"Target\"]).sum().reset_index()\n",
    "    edges.to_csv(\"ppl_plc_Edges.csv\",index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardize People's Names -- One to Many Transformation\n",
    "## arg: nonstd = Series of non-standard names\n",
    "## return standardized names in a Series\n",
    "def StdNm (nonstd):\n",
    "    \n",
    "    ## make a DataFrame indexed by standardized name and contains columns \"FirstName\", \"OtherNames\", \"OtherNames1\"\n",
    "    std=pd.read_csv(\"StandardNames.csv\")\n",
    "    std[\"FullName\"]=std[\"LastName\"]+std[\"FirstName\"]\n",
    "    std[\"FullName\"].fillna(method=\"ffill\",inplace=True)\n",
    "    std.set_index(\"FullName\",inplace=True)\n",
    "    std[\"OtherNames1\"]=std[\"LastName\"].fillna(method=\"ffill\")+std['OtherNames']\n",
    "    std.drop([\"Details\",\"Studio\",\"LastName\"],axis=1,inplace=True)\n",
    "    \n",
    "    ## make a dictionary of {key=standardized name: value=np.array of alternative names}\n",
    "    dNames={}\n",
    "    for stdNm in std.index.unique():\n",
    "        if np.ndim(std.ix[stdNm])==1:\n",
    "            arr=std.ix[stdNm].values\n",
    "            dNames[stdNm]=arr[pd.notnull(arr)]\n",
    "        else:\n",
    "            arr=std.ix[stdNm].stack().values\n",
    "            dNames[stdNm]=arr[pd.notnull(arr)]\n",
    "    \n",
    "    ## Standardize names in the given Series\n",
    "    for idx, nonstdNm in nonstd.dropna().iteritems():\n",
    "        for stdNm in dNames.keys():\n",
    "            for alt in dNames[stdNm]:\n",
    "                if (alt in nonstdNm) & (nonstdNm not in dNames.keys()):\n",
    "                    nonstd[idx]=nonstdNm.replace(alt,stdNm)\n",
    "    \n",
    "    return nonstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
